{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual of DATE (Duat-Attentive-Tree-aware-Embedding) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Updated on: 2020. 6. 3.\n",
    "* Written by Yeon Soo Choi, Technical Officer, Research Unit, WCO\n",
    "* Original research paper\n",
    "    * Title: DATE: Dual Attentive Tree-aware Embedding for Customs Frauds Detection\n",
    "    * Authors\n",
    "        - IBS: Sundong Kim, Karandeep Singh, Meeyoung Cha\n",
    "        - NCKU: Yu-Che Tsai, Cheng-Te Li\n",
    "        - WCO: Yeon Soo Choi\n",
    "        - NCS: Etim Ibok\n",
    "    * Source: https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding\n",
    "        - All the python codes are included in this one notebook.\n",
    "* Data: T import data (xxx at transaction-item level)\n",
    "\n",
    "\n",
    "## Summary (in non-technical terms)\n",
    "\n",
    "**DATE (Dual-Attentive-Tree-aware-Embedding)** is the neural networks model to detect undervalued imports.  \n",
    "\n",
    "Imagine that you **(\"neural networks\")** are the head of a Customs targeting centre composed of **100 risk analysts (\"decision trees\")**. For a given import, you task the analysts with reporting **the probability of undervaluation** and **the estimate of additional revenue from the inspection (\"dual-task\")**.  \n",
    "\n",
    "How would you put 100 different reports together in making your final decision?\n",
    "Simply averaging their predictions may neglect some valuable information hidden in 100 reports. The DATE model help you keep all the information while paying more **ATTENTION** to specific pieces. Firstly, if there are a majority group of reports significantly similar to each other, you may pay more **ATTENTION** to those reports. Secondly, if you have analysts specialized in the specific HS code and importer of the given import, you may pay more **ATTENTION** to their reports. In the end, your final decision reflects the reports attracting your higher attention. \n",
    "\n",
    "WCO press release is available at:  \n",
    "http://www.wcoomd.org/en/media/newsroom/2020/may/wco-bacuda-experts-develop-and-share-a-neural-network-model.aspx\n",
    "\n",
    "## Summary (in technical terms)\n",
    "\n",
    "Now, lets take an overview of the model with some technical terms. For a given import, the DATE model works in the following steps;\n",
    "\n",
    "* **XGBoost**: The model passes the import into a XGBoost model which constructs multiple(eg. 100) decision trees. \n",
    "* **Embedding**: Each tree's decision (decision path, leaf-id) is transformed into a set of numbers to be fed into neural networks. \n",
    "* **Multi-head Self-attention**\n",
    "    - Self-attention: The numeric value (importance, weight) of each leaf-id is adjusted based on its correlation/interaction with other leaf-ids.\n",
    "    - Multi-head: Self-iteration is repeated in multiple times to achieve its robustness.\n",
    "* **Attention**: The numeric values (importance, weight) of each leaf-id is re-adjusted based on its correlation/interaction with the given importer-id and item-id(HScode).\n",
    "\n",
    "## OUTLINE\n",
    "* [Part 1. Preprocess data](#part1)\n",
    "* [Part 2. XGBoost model](#part2)\n",
    "* [Part 3. DATE (XGBoost + Neural Networks + Attention)](#part3)\n",
    "* [Part 4. Evaluation](#part4)\n",
    "* [Part 5. Comparison with XGBoost + Logistic Regression model](#part5)\n",
    "* [Part 6. Practice of functions](#part6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preprocess <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "pd.set_option('display.max_columns',100)\n",
    "from collections import defaultdict\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load and clean data\n",
    "### 1.2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('./synthetic_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN',\n",
       "       'TARIFF.CODE', 'ORIGIN.CODE', 'CIF_USD_EQUIVALENT', 'QUANTITY',\n",
       "       'GROSS.WEIGHT', 'TOTAL.TAXES.USD', 'RAISED_TAX_AMOUNT_USD', 'illicit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"CIF_USD_EQUIVALENT\": \"CIF\", \n",
    "                        \"TOTAL.TAXES.USD\": \"TOTAL.TAXES\",\n",
    "                        \"RAISED_TAX_AMOUNT_USD\":'RAISED_TAX_AMOUNT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>OFFICE</th>\n",
       "      <th>IMPORTER.TIN</th>\n",
       "      <th>TARIFF.CODE</th>\n",
       "      <th>ORIGIN.CODE</th>\n",
       "      <th>CIF</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>GROSS.WEIGHT</th>\n",
       "      <th>TOTAL.TAXES</th>\n",
       "      <th>RAISED_TAX_AMOUNT</th>\n",
       "      <th>illicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>OFFICE168</td>\n",
       "      <td>IMPO19832</td>\n",
       "      <td>3926909710</td>\n",
       "      <td>CNTRY994</td>\n",
       "      <td>19056.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>OFFICE296</td>\n",
       "      <td>IMPO10210</td>\n",
       "      <td>8704319941</td>\n",
       "      <td>CNTRY959</td>\n",
       "      <td>532.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>OFFICE298</td>\n",
       "      <td>IMPO11255</td>\n",
       "      <td>5806200000</td>\n",
       "      <td>CNTRY994</td>\n",
       "      <td>11.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>OFFICE168</td>\n",
       "      <td>IMPO16962</td>\n",
       "      <td>5807109010</td>\n",
       "      <td>CNTRY376</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>822.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>OFFICE109</td>\n",
       "      <td>IMPO10548</td>\n",
       "      <td>3204110000</td>\n",
       "      <td>CNTRY976</td>\n",
       "      <td>86099.0</td>\n",
       "      <td>333634.0</td>\n",
       "      <td>3606.0</td>\n",
       "      <td>10166.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  day     OFFICE IMPORTER.TIN  TARIFF.CODE ORIGIN.CODE  \\\n",
       "199995  2014     12   31  OFFICE168    IMPO19832   3926909710    CNTRY994   \n",
       "199996  2014     12   31  OFFICE296    IMPO10210   8704319941    CNTRY959   \n",
       "199997  2014     12   31  OFFICE298    IMPO11255   5806200000    CNTRY994   \n",
       "199998  2014     12   31  OFFICE168    IMPO16962   5807109010    CNTRY376   \n",
       "199999  2014     12   31  OFFICE109    IMPO10548   3204110000    CNTRY976   \n",
       "\n",
       "            CIF  QUANTITY  GROSS.WEIGHT  TOTAL.TAXES  RAISED_TAX_AMOUNT  \\\n",
       "199995  19056.0       1.0         189.0          5.0               -0.0   \n",
       "199996    532.0     243.0          15.0        234.0              105.0   \n",
       "199997     11.0      58.0          24.0          5.0               -0.0   \n",
       "199998   2826.0       3.0         822.0          5.0               -0.0   \n",
       "199999  86099.0  333634.0        3606.0      10166.0              267.0   \n",
       "\n",
       "        illicit  \n",
       "199995        0  \n",
       "199996        1  \n",
       "199997        0  \n",
       "199998        0  \n",
       "199999        1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Define functions to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. \"merge_attributes\" function [(link to practice)](#practice1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"merge_attributes\" fuction is to create a new categorical variable by combining multiple existing categorical variables into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_attributes(df: pd.DataFrame, *args: str) -> None: \n",
    "    # Note: \"*args\" represents multiple arguments, i.e. multiple variable names could come. \n",
    "    # Note: \"-> None\" represents that this function returns None (i.e. type annotation)\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype *args: strings (attribute names that want to be combined)\n",
    "    \"\"\"\n",
    "    # To set data type of each argument as string\n",
    "    iterables = [df[arg].astype(str) for arg in args] \n",
    "    # To name the newly combined variable/column\n",
    "    columnName = '&'.join([*args]) \n",
    "    # To create a column for the combined variable\n",
    "    fs = [''.join([v for v in var]) for var in zip(*iterables)] # \"*\" represents \"unzip\"\n",
    "    df.loc[:, columnName] = fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. \"preprocess\" function [(link to practice)](#practice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fuction is to;\n",
    "* generate additional features such as unitprice, weight-unitprice and effective tariff rate;\n",
    "* merge some attributes, using the above \"merge_attributes\" function; and \n",
    "* generate date-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Note: \"-> pd.DataFrame\" represents that this function returns a dataframe.\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    rtype df: dataframe\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['CIF', 'TOTAL.TAXES'])\n",
    "    df.loc[:, 'Unitprice'] = df['CIF']/df['QUANTITY']\n",
    "    df.loc[:, 'WUnitprice'] = df['CIF']/df['GROSS.WEIGHT']\n",
    "    df.loc[:, 'TaxRatio'] = df['TOTAL.TAXES'] / df['CIF']\n",
    "    df.loc[:, 'TaxUnitquantity'] = df['TOTAL.TAXES'] / df['QUANTITY']\n",
    "    df.loc[:, 'HS6'] = df['TARIFF.CODE'].apply(lambda x: int(x // 10000)) #HS10digit\n",
    "    df.loc[:, 'HS4'] = df['HS6'].apply(lambda x: int(x // 100))\n",
    "    df.loc[:, 'HS2'] = df['HS4'].apply(lambda x: int(x // 100))\n",
    "    \n",
    "    # Made a general function \"merge_attributes\" for supporting any combination    \n",
    "    merge_attributes(df, 'HS6','ORIGIN.CODE')\n",
    "    merge_attributes(df, 'OFFICE','IMPORTER.TIN')\n",
    "    merge_attributes(df, 'OFFICE','HS6')\n",
    "    merge_attributes(df, 'OFFICE','ORIGIN.CODE')\n",
    "    # another way of combining features\n",
    "    #df.loc[:, 'HS6.Origin'] = [str(i)+'&'+j for i, j in zip(df['HS6'], df['ORIGIN'])]\n",
    "    \n",
    "    \n",
    "    '''# Day of Year of SGD.DATE\n",
    "    tmp2 = {}\n",
    "    for date in set(df['SGD.DATE']):\n",
    "        tmp2[date] = dt.strptime(date, '%Y%m%d') # Capital letter Y\n",
    "    tmp_day = {}\n",
    "    tmp_week = {}\n",
    "    tmp_month = {}\n",
    "    yearStart = dt(tmp2[date].date().year, 1, 1)\n",
    "    for item in tmp2:\n",
    "        tmp_day[item] = (tmp2[item] - yearStart).days\n",
    "        tmp_week[item] = int(tmp_day[item] / 7)\n",
    "        tmp_month[item] = int(tmp_day[item] / 30)\n",
    "        \n",
    "    df.loc[:, 'SGD.DayofYear'] = df['SGD.DATE'].apply(lambda x: tmp_day[x])\n",
    "    df.loc[:, 'SGD.WeekofYear'] = df['SGD.DATE'].apply(lambda x: tmp_week[x])\n",
    "    df.loc[:, 'SGD.MonthofYear'] = df['SGD.DATE'].apply(lambda x: tmp_month[x])'''\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. \"find_risk_profile\" function [(link to practice)](#practice3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to identify/calculate risk-profiling indicators of the features;\n",
    "* option 1 (topk): Lists of top n high-risk categories in the features\n",
    "* option 2 (ratio): illicit ratio of categories in the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk_profile(df: pd.DataFrame, \n",
    "                      feature: str, \n",
    "                      topk_ratio: float, \n",
    "                      adj: float, \n",
    "                      option: str) -> list or dict:\n",
    "    \"\"\"\n",
    "    dtype feature: str\n",
    "    dtype topk_ratio: float (range: 0-1)\n",
    "    dtype adj: float (to modify the mean)\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: list(option='topk') or dict(option='ratio')\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        # Group data by a specified feature(vairable)\n",
    "        total_cnt = df.groupby([feature])['illicit']\n",
    "        # Set the number of entities to be included a black list.\n",
    "        nrisky_profile = int(topk_ratio*len(total_cnt))+1\n",
    "        # For each entity, calculate 'total number of frauds' divided by 'total number of imports' \n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return list(adj_prob_illicit.sort_values(ascending=False).head(nrisky_profile).index)\n",
    "    \n",
    "    # Illicit-ratio encoding (Mean target encoding)\n",
    "    # Refer: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html\n",
    "    # Refer: https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0\n",
    "    elif option == 'ratio':\n",
    "        # For target encoding, we just use 70% of train data to avoid overfitting (otherwise, test AUC drops significantly)\n",
    "        total_cnt = df.sample(frac=0.7).groupby([feature])['illicit']\n",
    "        # prob_illicit = total_cnt.mean()  # Simple mean\n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return adj_prob_illicit.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. \"tag_risky_profiles\" function [(link to practice)](#practice4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to generate risk-profiling tags of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_risky_profiles(df: pd.DataFrame, \n",
    "                       profile: str, \n",
    "                       profiles: list or dict, \n",
    "                       option: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype profile: str\n",
    "    dtype profiles: list(option='topk') or dictionary(option='ratio')\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: dataframe\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        d = defaultdict(int) # return 0 for not-defined keys\n",
    "        for id in profiles:\n",
    "            d[id] = 1\n",
    "    #     print(list(islice(d.items(), 10)))  # For debugging\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: d[x])\n",
    "    \n",
    "    # Illicit-ratio encoding\n",
    "    elif option == 'ratio':\n",
    "        overall_ratio_train = np.mean(train.illicit) # When scripting, saving it as a class variable is clearer.\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: profiles.get(x,overall_ratio_train))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.4. Preprocess data with pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset settings\n",
    "data_length = df.shape[0]\n",
    "train_ratio = 0.6\n",
    "valid_ratio = 0.8\n",
    "train_length = int(data_length*train_ratio)\n",
    "valid_length = int(data_length*valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/valid/test set\n",
    "#df = df.sort_values('SGD.DATE')\n",
    "train = df.iloc[:train_length,:]\n",
    "valid = df.iloc[train_length:valid_length,:]\n",
    "test = df.iloc[valid_length:,:]\n",
    "#train = df[df[\"SGD.DATE\"] < \"2015-12-01\"]\n",
    "#valid = df[(df[\"SGD.DATE\"] >= \"2015-12-01\") & (df[\"SGD.DATE\"] < \"2016-01-01\")]\n",
    "#test = df[df[\"SGD.DATE\"] >= \"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120000, 13), (40000, 13), (40000, 13))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'ORIGIN.CODE', 'CIF', 'QUANTITY', 'GROSS.WEIGHT', 'TOTAL.TAXES',\n",
       "       'RAISED_TAX_AMOUNT', 'illicit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label data\n",
    "train_reg_label = train['RAISED_TAX_AMOUNT'].values\n",
    "valid_reg_label = valid['RAISED_TAX_AMOUNT'].values\n",
    "test_reg_label = test['RAISED_TAX_AMOUNT'].values\n",
    "train_cls_label = train[\"illicit\"].values\n",
    "valid_cls_label = valid[\"illicit\"].values\n",
    "test_cls_label = test[\"illicit\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "train = preprocess(train)\n",
    "valid = preprocess(valid)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'ORIGIN.CODE', 'CIF', 'QUANTITY', 'GROSS.WEIGHT', 'TOTAL.TAXES',\n",
       "       'RAISED_TAX_AMOUNT', 'illicit', 'Unitprice', 'WUnitprice', 'TaxRatio',\n",
       "       'TaxUnitquantity', 'HS6', 'HS4', 'HS2', 'HS6&ORIGIN.CODE',\n",
       "       'OFFICE&IMPORTER.TIN', 'OFFICE&HS6', 'OFFICE&ORIGIN.CODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few more risky profiles\n",
    "risk_profiles = {}\n",
    "profile_candidates = ['IMPORTER.TIN',\n",
    "                      'ORIGIN.CODE', #'LAST_DEPARTURE.CODE', 'CONTRACT_PARTY.CODE',\n",
    "                      'TARIFF.CODE', 'QUANTITY', 'HS6', 'HS4', 'HS2', 'OFFICE'] + [col for col in train.columns if '&' in col]\n",
    "\n",
    "for profile in profile_candidates:\n",
    "    option = 'topk'\n",
    "    risk_profiles[profile] = find_risk_profile(train, profile, 0.1, 10, option=option)\n",
    "    train = tag_risky_profiles(train, profile, risk_profiles[profile], option=option)\n",
    "    valid = tag_risky_profiles(valid, profile, risk_profiles[profile], option=option)\n",
    "    test = tag_risky_profiles(test, profile, risk_profiles[profile], option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'ORIGIN.CODE', 'CIF', 'QUANTITY', 'GROSS.WEIGHT', 'TOTAL.TAXES',\n",
       "       'RAISED_TAX_AMOUNT', 'illicit', 'Unitprice', 'WUnitprice', 'TaxRatio',\n",
       "       'TaxUnitquantity', 'HS6', 'HS4', 'HS2', 'HS6&ORIGIN.CODE',\n",
       "       'OFFICE&IMPORTER.TIN', 'OFFICE&HS6', 'OFFICE&ORIGIN.CODE',\n",
       "       'RiskH.IMPORTER.TIN', 'RiskH.ORIGIN.CODE', 'RiskH.TARIFF.CODE',\n",
       "       'RiskH.QUANTITY', 'RiskH.HS6', 'RiskH.HS4', 'RiskH.HS2', 'RiskH.OFFICE',\n",
       "       'RiskH.HS6&ORIGIN.CODE', 'RiskH.OFFICE&IMPORTER.TIN',\n",
       "       'RiskH.OFFICE&HS6', 'RiskH.OFFICE&ORIGIN.CODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use in a classifier\n",
    "column_to_use = ['CIF', 'TOTAL.TAXES', 'GROSS.WEIGHT', 'QUANTITY',  \n",
    "                 'Unitprice', 'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'TARIFF.CODE', \n",
    "                 'HS6', 'HS4', 'HS2', 'year', 'month', 'day'] + [col for col in train.columns if 'RiskH' in col] \n",
    "\n",
    "# Extract only numeric values from data to be fed into models\n",
    "X_train = train[column_to_use].values\n",
    "X_valid = valid[column_to_use].values\n",
    "X_test = test[column_to_use].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute nan\n",
    "X_train = np.nan_to_num(X_train, 0)\n",
    "X_valid = np.nan_to_num(X_valid, 0)\n",
    "X_test = np.nan_to_num(X_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data size...\n",
      "(120000, 27) (120000,) (120000,)\n",
      "(40000, 27) (40000,) (40000,)\n",
      "(40000, 27) (40000,) (40000,)\n"
     ]
    }
   ],
   "source": [
    "# make sure the data size are correct\n",
    "print(\"Checking data size...\")\n",
    "print(X_train.shape, train_cls_label.shape, train_reg_label.shape)\n",
    "print(X_valid.shape, valid_cls_label.shape, valid_reg_label.shape)\n",
    "print(X_test.shape, test_cls_label.shape, test_reg_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Save all the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all data in a dictionary\n",
    "all_data = {\"raw\":{\"train\":train,\"valid\":valid,\"test\":test},\n",
    " \"xgboost_data\":{\"train_x\":X_train,\"train_y\":train_cls_label,\\\n",
    "                 \"valid_x\":X_valid,\"valid_y\":valid_cls_label,\\\n",
    "                 \"test_x\":X_test,\"test_y\":test_cls_label},\n",
    " \"revenue\":{\"train\":train_reg_label,\"valid\":valid_reg_label,\"test\":test_reg_label}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking label distribution\n",
      "Training: 0.07411385606874328\n",
      "Validation: 0.07883593602502899\n",
      "Testing: 0.08598267857627671\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Checking label distribution\")\n",
    "cnt = Counter(train_cls_label)\n",
    "print(\"Training:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(valid_cls_label)\n",
    "print(\"Validation:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(test_cls_label)\n",
    "print(\"Testing:\",cnt[1]/cnt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a variable to a file\n",
    "# reference for pickle: https://www.datacamp.com/community/tutorials/pickle-python-tutorial\n",
    "file = open('./processed_data.pickle', 'wb')\n",
    "pickle.dump(all_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. XGBoost model <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install xgboost -> https://stackoverflow.com/questions/55332943/importerror-no-module-named-xgboost\n",
    "# Install pytorch -> https://pytorch.org/get-started/locally/#mac-anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import copy\n",
    "import os \n",
    "from xgboost import XGBClassifier\n",
    "#(toberemoved)from utils import find_best_threshold,process_leaf_idx,stratify_sample\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load the preprocessed data in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['raw', 'xgboost_data', 'revenue'])\n",
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data\n",
    "# with open(\"./processed_data_13-01-01.pickle\",\"rb\") as f :\n",
    "#     processed_data = pickle.load(f)\n",
    "with open(\"./processed_data.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)\n",
    "print(processed_data.keys())\n",
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data \n",
    "train = processed_data[\"raw\"][\"train\"]\n",
    "valid = processed_data[\"raw\"][\"valid\"]\n",
    "test = processed_data[\"raw\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split labels into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue data for regression target \n",
    "revenue_train = processed_data[\"revenue\"][\"train\"]\n",
    "revenue_valid = processed_data[\"revenue\"][\"valid\"]\n",
    "revenue_test = processed_data[\"revenue\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take logged values of revenue, and normalize them. --> getting more densed distribution/minimizing outliers' impacts.  \n",
    "As we assume no information on valid-data and test-data, they are normalized with the information of train-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize revenue by f(x) = log(x+1)/max(xi)\n",
    "norm_revenue_train = np.log(revenue_train+1)\n",
    "norm_revenue_valid = np.log(revenue_valid+1)\n",
    "norm_revenue_test = np.log(revenue_test+1) \n",
    "global_max = max(norm_revenue_train) \n",
    "norm_revenue_train = norm_revenue_train/global_max\n",
    "norm_revenue_valid = norm_revenue_valid/global_max\n",
    "norm_revenue_test = norm_revenue_test/global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split xgboost data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost data \n",
    "xgb_trainx = processed_data[\"xgboost_data\"][\"train_x\"]\n",
    "xgb_trainy = processed_data[\"xgboost_data\"][\"train_y\"]\n",
    "xgb_validx = processed_data[\"xgboost_data\"][\"valid_x\"]\n",
    "xgb_validy = processed_data[\"xgboost_data\"][\"valid_y\"]\n",
    "xgb_testx = processed_data[\"xgboost_data\"][\"test_x\"]\n",
    "xgb_testy = processed_data[\"xgboost_data\"][\"test_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Define functions to be used\n",
    "### 2.3.1. \"find_best_threshod\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(model,x_list,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    The input arguments are;\n",
    "    - dtype model: scikit-learn classifier model\n",
    "    - dtype x_list: list or array of features (data)\n",
    "    - dtype y_test: array of true labels \n",
    "    '''\n",
    "    # Predict probability of fraud of each import.\n",
    "    y_pred_prob = model.predict_proba(x_list)[:,1]\n",
    "    # Set threshold range as [0.1, 0.2, ..., 0.5]. \n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    # Set an initial value of best threshold.\n",
    "    best_f1 = 0\n",
    "    # if best_thresh is set as \"None\", this function is to find the best_thresh as well as best_f1 \n",
    "    if best_thresh ==None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1\n",
    "    # if best_thresh is set as a certain number, this function is to calculate its f1 score.\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "    print(\"F1-scre equals to:%.4f\"%(best_f1))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Deploy a XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xgboost model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training xgboost model...\")\n",
    "# Convert nparray to pd.DataFrame (why? not necessary)\n",
    "columns = column_to_use\n",
    "xgb_trainx = pd.DataFrame(xgb_trainx,columns=columns)\n",
    "xgb_validx = pd.DataFrame(xgb_validx,columns=columns)\n",
    "xgb_testx = pd.DataFrame(xgb_testx,columns=columns)\n",
    "# Initiate the model\n",
    "xgb_clf = XGBClassifier(n_estimators=100, max_depth=4,n_jobs=-1)\n",
    "# Train/fit the model\n",
    "xgb_clf.fit(xgb_trainx,xgb_trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first tree out of 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install graphviz -> https://anaconda.org/conda-forge/python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"/newsdd/bacuda_big/anaconda3/lib/graphviz/libgvplugin_pango.so.6\" - file not found\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"2553pt\" height=\"399pt\"\n",
       " viewBox=\"0.00 0.00 2553.35 399.20\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 395.2)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-395.2 2549.3491,-395.2 2549.3491,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1341.8677\" cy=\"-373.2\" rx=\"163.9781\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1341.8677\" y=\"-369\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.OFFICE&amp;IMPORTER.TIN&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1049.8677\" cy=\"-284.4\" rx=\"142.6152\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1049.8677\" y=\"-280.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.HS6&amp;ORIGIN.CODE&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1285.9127,-356.1835C1236.9071,-341.2805 1165.6959,-319.6245 1114.3192,-304.0003\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1115.2298,-300.619 1104.6441,-301.058 1113.1931,-307.3162 1115.2298,-300.619\"/>\n",
       "<text text-anchor=\"middle\" x=\"1257.4778\" y=\"-324.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1684.8677\" cy=\"-284.4\" rx=\"143.9017\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1684.8677\" y=\"-280.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TARIFF.CODE&lt;8.60900557e+09</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1405.981,-356.6016C1464.5732,-341.4325 1551.1475,-319.0191 1612.2652,-303.1962\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1613.423,-306.5119 1622.2266,-300.6173 1611.6686,-299.7354 1613.423,-306.5119\"/>\n",
       "<text text-anchor=\"middle\" x=\"1551.8677\" y=\"-324.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"586.8677\" cy=\"-195.6\" rx=\"113.6789\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"586.8677\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.OFFICE&amp;HS6&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M971.3261,-269.3363C887.3193,-253.2244 754.6724,-227.7837 669.2264,-211.3958\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"669.7852,-207.9392 659.3049,-209.4929 668.4666,-214.8139 669.7852,-207.9392\"/>\n",
       "<text text-anchor=\"middle\" x=\"894.4778\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1049.8677\" cy=\"-195.6\" rx=\"143.9017\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1049.8677\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TARIFF.CODE&lt;8.70200115e+09</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1049.8677,-266.0006C1049.8677,-253.8949 1049.8677,-237.8076 1049.8677,-224.0674\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1053.3678,-223.672 1049.8677,-213.672 1046.3678,-223.6721 1053.3678,-223.672\"/>\n",
       "<text text-anchor=\"middle\" x=\"1056.8677\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1684.8677\" cy=\"-195.6\" rx=\"113.6789\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1684.8677\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.OFFICE&amp;HS6&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;5 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>2&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1684.8677,-266.0006C1684.8677,-253.8949 1684.8677,-237.8076 1684.8677,-224.0674\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1688.3678,-223.672 1684.8677,-213.672 1681.3678,-223.6721 1688.3678,-223.672\"/>\n",
       "<text text-anchor=\"middle\" x=\"1719.4778\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>6</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2000.8677\" cy=\"-195.6\" rx=\"119.6167\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2000.8677\" y=\"-191.4\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TARIFF.CODE&lt;8.704e+09</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1743.5643,-267.9055C1797.6973,-252.6934 1877.9838,-230.1319 1934.4441,-214.2659\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1935.6653,-217.5584 1944.3455,-211.4835 1933.7715,-210.8194 1935.6653,-217.5584\"/>\n",
       "<text text-anchor=\"middle\" x=\"1878.8677\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"272.8677\" cy=\"-106.8\" rx=\"122.9287\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.IMPORTER.TIN&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M531.0926,-179.8267C477.3658,-164.6326 396.0686,-141.6415 339.0824,-125.5257\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"340.0214,-122.154 329.4464,-122.8006 338.1165,-128.8899 340.0214,-122.154\"/>\n",
       "<text text-anchor=\"middle\" x=\"493.4778\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"586.8677\" cy=\"-106.8\" rx=\"122.9287\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"586.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.IMPORTER.TIN&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;8 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>3&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M586.8677,-177.2006C586.8677,-165.0949 586.8677,-149.0076 586.8677,-135.2674\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"590.3678,-134.872 586.8677,-124.872 583.3678,-134.8721 590.3678,-134.872\"/>\n",
       "<text text-anchor=\"middle\" x=\"593.8677\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>9</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"950.8677\" cy=\"-106.8\" rx=\"158.8035\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"950.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.OFFICE&amp;ORIGIN.CODE&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;9 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>4&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1029.8336,-177.63C1015.0729,-164.3901 994.8161,-146.2204 978.5152,-131.5989\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"980.6532,-128.815 970.872,-124.7433 975.9792,-134.0259 980.6532,-128.815\"/>\n",
       "<text text-anchor=\"middle\" x=\"1044.4778\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1250.8677\" cy=\"-106.8\" rx=\"122.9287\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1250.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.IMPORTER.TIN&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;10 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>4&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1089.0997,-178.2677C1121.5772,-163.9194 1167.8587,-143.4727 1202.6037,-128.1226\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1204.3439,-131.1802 1212.0766,-123.9376 1201.5151,-124.7772 1204.3439,-131.1802\"/>\n",
       "<text text-anchor=\"middle\" x=\"1175.8677\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>15</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"84.8677\" cy=\"-18\" rx=\"84.7356\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"84.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.193787798</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;15 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M236.1731,-89.4677C205.5369,-74.997 161.7683,-54.3233 129.1806,-38.9308\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"130.4173,-35.6442 119.8804,-34.5379 127.4276,-41.9736 130.4173,-35.6442\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>16</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"272.8677\" cy=\"-18\" rx=\"84.7356\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"272.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.148708925</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;16 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M272.8677,-88.4006C272.8677,-76.2949 272.8677,-60.2076 272.8677,-46.4674\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"276.3678,-46.072 272.8677,-36.072 269.3678,-46.0721 276.3678,-46.072\"/>\n",
       "<text text-anchor=\"middle\" x=\"279.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>17</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"455.8677\" cy=\"-18\" rx=\"80.0785\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"455.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.06932953</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;17 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M560.6728,-89.0434C540.2387,-75.1919 511.7058,-55.8505 489.5701,-40.8456\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"491.3684,-37.8363 481.127,-35.1224 487.4407,-43.6305 491.3684,-37.8363\"/>\n",
       "<text text-anchor=\"middle\" x=\"568.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>18</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"635.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"635.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.138095245</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;18 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>8&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M596.7836,-88.83C603.7573,-76.1919 613.2096,-59.062 621.0717,-44.814\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"624.1997,-46.3898 625.9666,-35.9433 618.0708,-43.0078 624.1997,-46.3898\"/>\n",
       "<text text-anchor=\"middle\" x=\"622.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>19</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"811.8677\" cy=\"-18\" rx=\"76.6493\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"811.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.12228968</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;19 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>9&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M922.3031,-88.9204C913.137,-83.1585 902.9509,-76.7296 893.6475,-70.8 878.187,-60.9461 861.1439,-49.9619 846.6359,-40.575\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"848.2632,-37.4591 837.967,-34.9611 844.4582,-43.3346 848.2632,-37.4591\"/>\n",
       "<text text-anchor=\"middle\" x=\"928.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>20</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"987.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"987.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.188916877</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;20 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>9&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M958.5341,-88.4006C963.6781,-76.0552 970.5473,-59.5689 976.3452,-45.6539\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"979.7223,-46.649 980.3377,-36.072 973.2607,-43.9566 979.7223,-46.649\"/>\n",
       "<text text-anchor=\"middle\" x=\"979.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>21</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1172.8677\" cy=\"-18\" rx=\"84.7356\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1172.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=&#45;0.106336094</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;21 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>10&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1235.0832,-88.83C1223.6651,-75.8308 1208.0727,-58.0795 1195.3541,-43.5999\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1197.8578,-41.1467 1188.6287,-35.9433 1192.5985,-45.7663 1197.8578,-41.1467\"/>\n",
       "<text text-anchor=\"middle\" x=\"1253.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>22</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1357.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1357.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.128571436</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;22 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>10&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1272.2636,-89.0434C1288.5528,-75.5249 1311.143,-56.7772 1329.0272,-41.9349\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1331.4136,-44.5028 1336.8736,-35.4232 1326.9432,-39.1162 1331.4136,-44.5028\"/>\n",
       "<text text-anchor=\"middle\" x=\"1321.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>11</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1543.8677\" cy=\"-106.8\" rx=\"142.6152\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1543.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.HS6&amp;ORIGIN.CODE&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1657.0106,-178.056C1635.138,-164.2809 1604.4937,-144.9815 1580.6129,-129.9417\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1582.4765,-126.979 1572.1495,-124.6115 1578.7461,-132.9023 1582.4765,-126.979\"/>\n",
       "<text text-anchor=\"middle\" x=\"1662.4778\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>12</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1785.8677\" cy=\"-106.8\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1785.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.198693678</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;12 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>5&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1705.3066,-177.63C1720.4645,-164.303 1741.3036,-145.9811 1757.9894,-131.3108\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1760.6049,-133.6717 1765.8039,-124.4402 1755.9828,-128.4147 1760.6049,-133.6717\"/>\n",
       "<text text-anchor=\"middle\" x=\"1751.8677\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>13</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2000.8677\" cy=\"-106.8\" rx=\"105.5343\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2000.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">RiskH.QUANTITY&lt;0.5</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>6&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2000.8677,-177.2006C2000.8677,-165.0949 2000.8677,-149.0076 2000.8677,-135.2674\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2004.3678,-134.872 2000.8677,-124.872 1997.3678,-134.8721 2004.3678,-134.872\"/>\n",
       "<text text-anchor=\"middle\" x=\"2035.4778\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>14</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2272.8677\" cy=\"-106.8\" rx=\"143.9017\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2272.8677\" y=\"-102.6\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TARIFF.CODE&lt;9.71049984e+09</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;14 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>6&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2051.0737,-179.2092C2096.3102,-164.4408 2162.951,-142.6846 2211.3698,-126.8773\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2212.6846,-130.1299 2221.1046,-123.6991 2210.5121,-123.4755 2212.6846,-130.1299\"/>\n",
       "<text text-anchor=\"middle\" x=\"2168.8677\" y=\"-147\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>23</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1538.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1538.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.159285709</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;23 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>11&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1542.8317,-88.4006C1542.1433,-76.175 1541.2263,-59.8887 1540.4477,-46.0599\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1543.942,-45.8594 1539.8853,-36.072 1536.9531,-46.253 1543.942,-45.8594\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1719.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1719.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.197989956</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>11&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1578.6396,-89.256C1607.1469,-74.8728 1647.5914,-54.4667 1677.8982,-39.1755\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1679.6024,-42.236 1686.9538,-34.6066 1676.4492,-35.9864 1679.6024,-42.236\"/>\n",
       "<text text-anchor=\"middle\" x=\"1654.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"1905.8677\" cy=\"-18\" rx=\"86.4628\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1905.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.0153034301</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>13&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1981.643,-88.83C1967.3856,-75.503 1947.7845,-57.1811 1932.0899,-42.5108\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1934.4351,-39.912 1924.7396,-35.6402 1929.655,-45.0258 1934.4351,-39.912\"/>\n",
       "<text text-anchor=\"middle\" x=\"1997.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2091.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2091.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.144736841</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>13&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2019.2829,-88.83C2032.8159,-75.6242 2051.3749,-57.5138 2066.3387,-42.9118\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2069.0778,-45.1293 2073.7904,-35.6402 2064.189,-40.1193 2069.0778,-45.1293\"/>\n",
       "<text text-anchor=\"middle\" x=\"2061.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2272.8677\" cy=\"-18\" rx=\"81.8061\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2272.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.185553059</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>14&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2272.8677,-88.4006C2272.8677,-76.2949 2272.8677,-60.2076 2272.8677,-46.4674\"/>\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2276.3678,-46.072 2272.8677,-36.072 2269.3678,-46.0721 2276.3678,-46.072\"/>\n",
       "<text text-anchor=\"middle\" x=\"2307.4778\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">yes, missing</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"2458.8677\" cy=\"-18\" rx=\"86.4628\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"2458.8677\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">leaf=0.0181818195</text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>14&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2309.6153,-89.256C2339.8722,-74.8108 2382.8535,-54.2907 2414.9271,-38.9781\"/>\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2416.5673,-42.0735 2424.0837,-34.6066 2413.5514,-35.7565 2416.5673,-42.0735\"/>\n",
       "<text text-anchor=\"middle\" x=\"2388.8677\" y=\"-58.2\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">no</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f9cc11b2fd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb.to_graphviz(booster = xgb_clf, num_trees=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Evaluating xgboost model------\n",
      "F1-scre equals to:0.1567\n",
      "AUC = 0.7227, F1-score = 0.1567\n"
     ]
    }
   ],
   "source": [
    "# evaluate xgboost model\n",
    "print(\"------Evaluating xgboost model------\")\n",
    "# Predict\n",
    "test_pred = xgb_clf.predict_proba(xgb_testx)[:,1]\n",
    "# Calculate auc\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "# Find the best threshold\n",
    "xgb_threshold,_ = find_best_threshold(xgb_clf, xgb_validx, xgb_validy)\n",
    "# Calculate the best f1 score\n",
    "xgb_f1 = find_best_threshold(xgb_clf, xgb_testx, xgb_testy,best_thresh=xgb_threshold)\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. DATE model (XGB + Neural Networks + Attention) <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Set environmenst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ranger -> https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "# You may restart your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model.AttTreeEmbedding import Attention, DATE\n",
    "from ranger import Ranger\n",
    "#from utils import torch_threshold, fgsm_attack, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Preprocessing data: Integer-encoding of IMPORTER.TIN and TARIFF.CODE for attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user & item information \n",
    "train_raw_importers = train['IMPORTER.TIN'].values\n",
    "train_raw_items = train['TARIFF.CODE'].values\n",
    "valid_raw_importers = valid['IMPORTER.TIN'].values\n",
    "valid_raw_items = valid['TARIFF.CODE'].values\n",
    "test_raw_importers = test['IMPORTER.TIN']\n",
    "test_raw_items = test['TARIFF.CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need padding for unseen user or item \n",
    "importer_set = set(train_raw_importers)\n",
    "item_set = set(train_raw_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to +1 for zero padding \n",
    "importer_mapping = {v:i+1 for i,v in enumerate(importer_set)} \n",
    "hs6_mapping = {v:i+1 for i,v in enumerate(item_set)}\n",
    "importer_size = len(importer_mapping) + 1\n",
    "item_size = len(hs6_mapping) + 1\n",
    "# label-encoding\n",
    "train_importers = [importer_mapping[x] for x in train_raw_importers]\n",
    "train_items = [hs6_mapping[x] for x in train_raw_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data, we use padding_idx=0 for unseen data\n",
    "# use dic.get(key,deafault) to handle unseen\n",
    "valid_importers = [importer_mapping.get(x,0) for x in valid_raw_importers]\n",
    "valid_items = [hs6_mapping.get(x,0) for x in valid_raw_items]\n",
    "test_importers = [importer_mapping.get(x,0) for x in test_raw_importers] \n",
    "test_items = [hs6_mapping.get(x,0) for x in test_raw_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. \"process_leaf_idx\" function [(link to practice)](#practice5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_leaf_idx(X_leaves): \n",
    "    '''\n",
    "    This function is to convert the output of XGBoost model to the input of DATE model.\n",
    "    For an individual import, the output of XGBoost model is a list of leaf index of multiple trees.\n",
    "    eg. [1, 1, 10, 9, 30, 30, 32, ... ]\n",
    "    How to distinguish \"node 1\" of the first tree from \"node 1\" of the second tree?\n",
    "    How to distinguish \"node 30\" of the fifth tree from \"node 30\" of the sixth tree?\n",
    "    This function is to assign unique index to every leaf node in all the trees. \n",
    "    This function returns;\n",
    "    - lists of unique leaf index;\n",
    "    - total number of unique leaf nodes; and\n",
    "    - a reference table (dictionary) composed of \"unique leaf index\", \"tree id\", \"(previous) leaf index\". \n",
    "    '''\n",
    "    leaves = X_leaves.copy()\n",
    "    new_leaf_index = dict() # dictionary to store leaf index\n",
    "    total_leaves = 0\n",
    "    for c in range(X_leaves.shape[1]): # iterate for each column (ie. 100 trees)\n",
    "        column = X_leaves[:,c]\n",
    "        unique_vals = list(sorted(set(column)))\n",
    "        new_idx = {v:(i+total_leaves) for i,v in enumerate(unique_vals)}\n",
    "        for i,v in enumerate(unique_vals):\n",
    "            leaf_id = i+total_leaves\n",
    "            new_leaf_index[leaf_id] = {c:v}\n",
    "        leaves[:,c] = [new_idx[v] for v in column]\n",
    "        total_leaves += len(unique_vals)\n",
    "        \n",
    "    assert leaves.ravel().max() == total_leaves - 1\n",
    "    return leaves,total_leaves,new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. \"fgsm_attack\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, loss, images, labels, eps) :\n",
    "    # images.requires_grad = True\n",
    "    images = Variable(images, requires_grad=True)\n",
    "    outputs = model.module.pred_from_hidden(images)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    cost = loss(outputs, labels)\n",
    "    cost.backward()\n",
    "    attack_images = images + eps * images.grad.sign()\n",
    "    # attack_images = images + eps * F.normalize(images.grad.data, dim=0, p=2)\n",
    "    # attack_images.requires_grad = False\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. \"metrics\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_prob,xgb_testy,revenue_test,best_thresh=None):\n",
    "    if best_thresh ==None:\n",
    "        _,overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "    else:\n",
    "        overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "     # Seized revenue \n",
    "    # Precision and Recall\n",
    "    pr, re, f, rev = [], [], [], []\n",
    "    for i in [99,98,95,90]:\n",
    "        threshold = np.percentile(y_prob, i)\n",
    "        #print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "        precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "        recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "\n",
    "        # save results\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "        f.append(f1)\n",
    "        rev.append(revenue_recall)\n",
    "        # print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')\n",
    "    return overall_f1,auc,pr, re, f, rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. \"torch_threshold\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_threshold(y_pred_prob,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    '''\n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    best_f1 = 0\n",
    "    if best_thresh == None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1, roc_auc_score(y_test, y_pred_prob)\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "        return best_f1, roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Identify leaf nodes of individual import from XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "train_rows = train.shape[0]\n",
    "valid_rows = valid.shape[0] + train_rows\n",
    "X_leaves = np.concatenate((X_train_leaves, X_valid_leaves, X_test_leaves), axis=0) # make sure the dimensionality\n",
    "transformed_leaves, leaf_num, new_leaf_index = process_leaf_idx(X_leaves)\n",
    "train_leaves, valid_leaves, test_leaves = transformed_leaves[:train_rows],\\\n",
    "                                          transformed_leaves[train_rows:valid_rows],\\\n",
    "                                          transformed_leaves[valid_rows:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Convert data to tensor\n",
    "Tensor is a collection of numbers with specific shape (dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch type\n",
    "train_leaves = torch.tensor(train_leaves).long()\n",
    "train_user = torch.tensor(train_importers).long()\n",
    "train_item = torch.tensor(train_items).long()\n",
    "\n",
    "valid_leaves = torch.tensor(valid_leaves).long()\n",
    "valid_user = torch.tensor(valid_importers).long()\n",
    "valid_item = torch.tensor(valid_items).long()\n",
    "\n",
    "test_leaves = torch.tensor(test_leaves).long()\n",
    "test_user = torch.tensor(test_importers).long()\n",
    "test_item = torch.tensor(test_items).long()\n",
    "\n",
    "# cls data\n",
    "train_label_cls = torch.tensor(xgb_trainy).float()\n",
    "valid_label_cls = torch.tensor(xgb_validy).float()\n",
    "test_label_cls = torch.tensor(xgb_testy).float()\n",
    "\n",
    "# revenue data \n",
    "train_label_reg = torch.tensor(norm_revenue_train).float()\n",
    "valid_label_reg = torch.tensor(revenue_valid).float()\n",
    "test_label_reg = torch.tensor(revenue_test).float()\n",
    "\n",
    "# create dataloader \n",
    "\n",
    "train_dataset = Data.TensorDataset(train_leaves,train_user,train_item,train_label_cls,train_label_reg)\n",
    "valid_dataset = Data.TensorDataset(valid_leaves,valid_user,valid_item,valid_label_cls,valid_label_reg)\n",
    "test_dataset = Data.TensorDataset(test_leaves,test_user,test_item,test_label_cls,test_label_reg)\n",
    "\n",
    "\n",
    "\n",
    "data4embedding = {\"train_dataset\":train_dataset,\"valid_dataset\":valid_dataset,\"test_dataset\":test_dataset,\\\n",
    "                  \"leaf_num\":leaf_num,\"importer_num\":importer_size,\"item_size\":item_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Save and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"torch_data_test.pickle\", 'wb') as f:\n",
    "    pickle.dump(data4embedding, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"leaf_index.pickle\", \"wb\") as f:\n",
    "    pickle.dump(new_leaf_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load torch dataset \n",
    "with open(\"torch_data_test.pickle\",\"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get torch dataset \n",
    "train_dataset = data[\"train_dataset\"]\n",
    "valid_dataset = data[\"valid_dataset\"]\n",
    "test_dataset = data[\"test_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=True,               \n",
    ")\n",
    "valid_loader = Data.DataLoader(\n",
    "    dataset=valid_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "\n",
    "# parameters for model \n",
    "leaf_num = data[\"leaf_num\"]\n",
    "importer_size = data[\"importer_num\"]\n",
    "item_size = data[\"item_size\"]\n",
    "\n",
    "# global variables\n",
    "xgb_validy = valid_loader.dataset.tensors[-2].detach().numpy()\n",
    "xgb_testy = test_loader.dataset.tensors[-2].detach().numpy()\n",
    "revenue_valid = valid_loader.dataset.tensors[-1].detach().numpy()\n",
    "revenue_test = test_loader.dataset.tensors[-1].detach().numpy()\n",
    "\n",
    "# model information\n",
    "curr_time = str(time.time())\n",
    "model_name = \"DATE\"\n",
    "model_path = \"./saved_models/%s%s.pkl\" % (model_name,curr_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch_multi_head_attention -> https://github.com/CyberZHG/torch-multi-head-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import numpy as np \n",
    "from torch_multi_head_attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Mish()](#practice6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mish,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x *( torch.tanh(F.softplus(x))) #softplus???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice FusionAttention()](#practice7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim):\n",
    "        super(FusionAttention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, dim) # nn.Linear(size of input, size of output)\n",
    "        self.project_weight = nn.Linear(dim,1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        query_project = self.attention_matrix(inputs) # (b,t,d) -> (b,t,d2)\n",
    "        query_project = F.leaky_relu(query_project)\n",
    "        project_value = self.project_weight(query_project) # (b,t,h) -> (b,t,1)\n",
    "        attention_weight = torch.softmax(project_value, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = inputs * attention_weight\n",
    "        attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Attention()](#practice8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim,hidden,aggregate=\"sum\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, hidden)\n",
    "        self.project_weight = nn.Linear(hidden*2,hidden)\n",
    "        self.h = nn.Parameter(torch.rand(hidden,1))\n",
    "        self.agg_type = aggregate\n",
    "        \n",
    "    def forward(self, query, key): # query: 256 X 16, # key: 256 X 100 X 16, # assume key==value\n",
    "        dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "        batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "        time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "        \n",
    "        # concate input query and key \n",
    "        query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "        query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "        cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "        \n",
    "        # project to single value\n",
    "        project_vector = self.project_weight(cat_vector) \n",
    "        project_vector = torch.relu(project_vector)\n",
    "        attention_alpha = torch.matmul(project_vector,self.h)\n",
    "        attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = key * attention_weight\n",
    "        \n",
    "        # aggregate leaves\n",
    "        if self.agg_type == \"max\":\n",
    "            attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"mean\":\n",
    "            attention_vec = torch.mean(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"sum\":\n",
    "            attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice DATE()](#practice9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_leaf,importer_size,item_size,dim,\n",
    "                 head_num=4,fusion_type=\"concat\",act=\"relu\",device=\"cpu\",use_self=True,agg_type=\"sum\"):\n",
    "        super(DATE, self).__init__()\n",
    "        self.d = dim\n",
    "        self.device = device\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.LeakyReLU()\n",
    "        elif act == \"mish\":\n",
    "            self.act = Mish() \n",
    "        self.fusion_type = fusion_type\n",
    "        self.use_self = use_self\n",
    "\n",
    "        # embedding layers \n",
    "        self.leaf_embedding = nn.Embedding(max_leaf,dim)\n",
    "        self.user_embedding = nn.Embedding(importer_size,dim,padding_idx=0)\n",
    "        self.user_embedding.weight.data[0] = torch.zeros(dim) # unseen data? initial value?\n",
    "        self.item_embedding = nn.Embedding(item_size,dim,padding_idx=0)\n",
    "        self.item_embedding.weight.data[0] = torch.zeros(dim)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_bolck = Attention(dim,dim,agg_type).to(device)\n",
    "        self.self_att = MultiHeadAttention(dim,head_num).to(device)\n",
    "        self.fusion_att = FusionAttention(dim)\n",
    "\n",
    "        # Hidden & output layer\n",
    "        self.layer_norm = nn.LayerNorm((100,dim))\n",
    "        self.fussionlayer = nn.Linear(dim*3,dim)\n",
    "        self.hidden = nn.Linear(dim,dim)\n",
    "        self.output_cls_layer = nn.Linear(dim,1)\n",
    "        self.output_reg_layer = nn.Linear(dim,1)\n",
    "    \n",
    "    def forward(self,feature,uid,item_id):\n",
    "        \n",
    "        # Embedding of leaf_id\n",
    "        leaf_vectors = self.leaf_embedding(feature)\n",
    "        \n",
    "        # 1st attention: Multi-Head Self-Attention\n",
    "        # Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "        if self.use_self:\n",
    "            # Apply multy head attention\n",
    "            leaf_vectors = self.self_att(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "            # Normalization?\n",
    "            leaf_vectors = self.layer_norm(leaf_vectors)\n",
    "        \n",
    "        # Embedding of importer_id\n",
    "        importer_vector = self.user_embedding(uid)\n",
    "        # Embedding of item_id\n",
    "        item_vector = self.item_embedding(item_id)\n",
    "        # Multiply embeddings of importer_id and item_id\n",
    "        query_vector = importer_vector * item_vector\n",
    "        \n",
    "        # 2nd attention: Attention with leaf_id, importer_id and item_id (all embeddings)\n",
    "        set_vector, self.attention_w = self.attention_bolck(query_vector,leaf_vectors)\n",
    "        \n",
    "        # concat the user, item and tree vectors into a fusion attention\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=-1) # attach as columns\n",
    "            fusion = self.act(self.fussionlayer(fusion))\n",
    "        elif self.fusion_type == \"attention\":\n",
    "            importer_vector=importer_vector.view(-1,1,self.d), \n",
    "            item_vector=item_vector.view(-1,1,self.d), \n",
    "            set_vector=set_vector.view(-1,1,self.d)\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "            fusion,_ = self.fusion_att(fusion)\n",
    "        else:\n",
    "            raise \"Fusion type error\"\n",
    "        hidden = self.hidden(fusion)\n",
    "        hidden = self.act(hidden)\n",
    "\n",
    "        # multi-task output \n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        regression_output = torch.relu(self.output_reg_layer(hidden))\n",
    "        return classification_output, regression_output, hidden\n",
    "\n",
    "    def pred_from_hidden(self,hidden):\n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        return classification_output \n",
    "\n",
    "    def eval_on_batch(self,test_loader): # predict test data using batch \n",
    "        final_output = []\n",
    "        cls_loss = []\n",
    "        reg_loss = []\n",
    "        for batch in test_loader:\n",
    "            batch_feature, batch_user, batch_item, batch_cls, batch_reg = batch\n",
    "            batch_feature,batch_user,batch_item,batch_cls,batch_reg =  \\\n",
    "            batch_feature.to(self.device), batch_user.to(self.device),\\\n",
    "            batch_item.to(self.device), batch_cls.to(self.device), batch_reg.to(self.device)\n",
    "            batch_cls,batch_reg = batch_cls.view(-1,1), batch_reg.view(-1,1)\n",
    "            y_pred_prob, y_pred_rev,_ = self.forward(batch_feature,batch_user,batch_item)\n",
    "\n",
    "            # compute classification loss\n",
    "            cls_losses = nn.BCELoss()(y_pred_prob,batch_cls)\n",
    "            cls_loss.append(cls_losses.item())\n",
    "\n",
    "            # compute regression loss \n",
    "            reg_losses = nn.MSELoss()(y_pred_rev, batch_reg)\n",
    "            reg_loss.append(reg_losses.item())\n",
    "\n",
    "            # store predicted probability \n",
    "            y_pred = y_pred_prob.detach().cpu().numpy().tolist()\n",
    "            final_output.extend(y_pred)\n",
    "\n",
    "        print(\"CLS loss: %.4f, REG loss: %.4f\"% (np.mean(cls_loss), np.mean(reg_loss)) )\n",
    "        return np.array(final_output).ravel(), np.mean(cls_loss)+ np.mean(reg_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Train (hyperparameter + loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.1. Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', \n",
    "                        type=str, \n",
    "                        default=\"DATE\", \n",
    "                        help=\"Name of model\",\n",
    "                        )\n",
    "parser.add_argument('--epoch', \n",
    "                        type=int, \n",
    "                        default=5, \n",
    "                        help=\"Number of epochs\",\n",
    "                        )\n",
    "parser.add_argument('--dim', \n",
    "                        type=int, \n",
    "                        default=16, \n",
    "                        help=\"Hidden layer dimension\",\n",
    "                        )\n",
    "parser.add_argument('--lr', \n",
    "                        type=float, \n",
    "                        default=0.005, \n",
    "                        help=\"learning rate\",\n",
    "                        )\n",
    "parser.add_argument('--l2',\n",
    "                        type=float,\n",
    "                        default=0.00,\n",
    "                        help=\"l2 reg\",\n",
    "                        )\n",
    "parser.add_argument('--alpha',\n",
    "                        type=float,\n",
    "                        default=10,\n",
    "                        help=\"Regression loss weight\",\n",
    "                        )\n",
    "parser.add_argument('--beta', type=float, default=0.00, help=\"Adversarial loss weight\")\n",
    "parser.add_argument('--head_num', type=int, default=4, help=\"Number of heads for self attention\")\n",
    "parser.add_argument('--use_self', type=int, default=1, help=\"Wheter to use self attention\")\n",
    "parser.add_argument('--fusion', type=str, choices=[\"concat\",\"attention\"], default=\"concat\", help=\"Fusion method for final embedding\")\n",
    "parser.add_argument('--agg', type=str, choices=[\"sum\",\"max\",\"mean\"], default=\"sum\", help=\"Aggreate type for leaf embedding\")\n",
    "parser.add_argument('--act', type=str, choices=[\"mish\",\"relu\"], default=\"relu\", help=\"Activation function\")\n",
    "parser.add_argument('--device', type=str, choices=[\"cuda:0\",\"cuda:1\",\"cpu\"], default=\"cuda:0\", help=\"device name for training\")\n",
    "parser.add_argument('--output', type=str, default=\"full.csv\", help=\"Name of output file\")\n",
    "parser.add_argument('--save', type=int, default=0, help=\"save model or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "# Why an empty list?\n",
    "# Refere to: https://stackoverflow.com/questions/50763033/argparse-in-jupyter-notebook-throws-a-typeerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory for saving results.\n",
    "import os\n",
    "if not os.path.exists('./saved_models'):\n",
    "    os.makedirs('./saved_models')\n",
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # get configs\n",
    "    epochs = args.epoch\n",
    "    dim = args.dim\n",
    "    lr = args.lr\n",
    "    weight_decay = args.l2\n",
    "    head_num = args.head_num\n",
    "    device = 'cpu' #args.device\n",
    "    act = args.act\n",
    "    fusion = args.fusion\n",
    "    beta = args.beta\n",
    "    alpha = args.alpha\n",
    "    use_self = args.use_self\n",
    "    agg = args.agg\n",
    "    model = DATE(leaf_num,importer_size,item_size,\\\n",
    "                                    dim,head_num,\\\n",
    "                                    fusion_type=fusion,act=act,device=device,\\\n",
    "                                    use_self=use_self,agg_type=agg,\n",
    "                                    ).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=[0,1])\n",
    "\n",
    "    # initialize parameters\n",
    "    # Fills the input Tensor with values according to the method described in \n",
    "    # Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), \n",
    "    # using a uniform distribution.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # optimizer & loss \n",
    "    optimizer = Ranger(model.parameters(), weight_decay=weight_decay,lr=lr)\n",
    "    cls_loss_func = nn.BCELoss()\n",
    "    reg_loss_func = nn.MSELoss()\n",
    "\n",
    "    # save best model\n",
    "    global_best_score = 0\n",
    "    model_state = None\n",
    "\n",
    "    # early stop settings \n",
    "    stop_rounds = 3\n",
    "    no_improvement = 0\n",
    "    current_score = None \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, (batch_feature,batch_user,batch_item,batch_cls,batch_reg) in enumerate(train_loader):\n",
    "            model.train() # prep to train model\n",
    "            batch_feature,batch_user,batch_item,batch_cls,batch_reg =  \\\n",
    "            batch_feature.to(device), batch_user.to(device), batch_item.to(device),\\\n",
    "             batch_cls.to(device), batch_reg.to(device)\n",
    "            batch_cls,batch_reg = batch_cls.view(-1,1), batch_reg.view(-1,1)\n",
    "\n",
    "            # model output\n",
    "            classification_output, regression_output, hidden_vector = model(batch_feature,batch_user,batch_item)\n",
    "\n",
    "            # FGSM attack\n",
    "            adv_vector = fgsm_attack(model,cls_loss_func,hidden_vector,batch_cls,0.01)\n",
    "            adv_output = model.module.pred_from_hidden(adv_vector) \n",
    "\n",
    "            # calculate loss\n",
    "            adv_loss_func = nn.BCELoss(weight=batch_cls) \n",
    "            adv_loss = beta * adv_loss_func(adv_output,batch_cls) \n",
    "            cls_loss = cls_loss_func(classification_output,batch_cls)\n",
    "            revenue_loss = alpha * reg_loss_func(regression_output, batch_reg)\n",
    "            loss = cls_loss + revenue_loss + adv_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 1000 ==0:  \n",
    "                print(\"CLS loss:%.4f, REG loss:%.4f, ADV loss:%.4f, Loss:%.4f\"\\\n",
    "                %(cls_loss.item(),revenue_loss.item(),adv_loss.item(),loss.item()))\n",
    "                \n",
    "        # evaluate \n",
    "        model.eval()\n",
    "        print(\"Validate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(valid_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "        overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_validy,revenue_valid)\n",
    "        select_best = np.mean(f1s)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" % (overall_f1, auc, select_best) )\n",
    "\n",
    "        print(\"Evaluate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(test_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_testy,revenue_test,best_thresh=best_threshold)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" %(overall_f1, auc, np.mean(f1s)) )\n",
    "\n",
    "        # save best model \n",
    "        if select_best > global_best_score:\n",
    "            global_best_score = select_best\n",
    "            torch.save(model,model_path)\n",
    "        \n",
    "         # early stopping \n",
    "        if current_score == None:\n",
    "            current_score = select_best\n",
    "            continue\n",
    "        if select_best < current_score:\n",
    "            current_score = select_best\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= stop_rounds:\n",
    "            print(\"Early stopping...\")\n",
    "            break \n",
    "        if select_best > current_score:\n",
    "            no_improvement = 0\n",
    "            current_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Evaluation <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(save_model):\n",
    "    print()\n",
    "    print(\"--------Evaluating DATE model---------\")\n",
    "    # create best model\n",
    "    best_model = torch.load(model_path)\n",
    "    best_model.eval()\n",
    "\n",
    "    # get threshold\n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(valid_loader)\n",
    "    best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "\n",
    "    # predict test \n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(test_loader)\n",
    "    overall_f1, auc, precisions, recalls, f1s, revenues = metrics(y_prob,xgb_testy,revenue_test,best_threshold)\n",
    "    best_score = f1s[0]\n",
    "    #os.system(\"rm %s\"%model_path)\n",
    "    if save_model:\n",
    "        scroed_name = \"./saved_models/%s_%.4f.pkl\" % (model_name,overall_f1)\n",
    "        torch.save(best_model,scroed_name)\n",
    "    \n",
    "    return overall_f1, auc, precisions, recalls, f1s, revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_f1, auc, precisions, recalls, f1s, revenues = evaluate(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save result\n",
    "output_file =  \"./results/full.csv\"\n",
    "print(\"Saving result...\",output_file)\n",
    "with open(output_file, 'a') as ff:\n",
    "    # print(args,file=ff)\n",
    "    print()\n",
    "    print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\" \\\n",
    "          % (overall_f1, auc,\\\n",
    "             precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "             recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "             revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "            ),\n",
    "        ) \n",
    "    output_metric = [16,overall_f1,auc] + precisions + recalls + revenues\n",
    "    output_metric = list(map(str,output_metric))\n",
    "    print(\" \".join(output_metric),file=ff)\n",
    "        \n",
    "    # print(\"Model:%s epoch:%d dim:%d lr:%f l2:%f beta:%f heads:%d fusion:%s activation:%s\"\n",
    "    #       % (model_name, epochs, dim, lr, weight_decay, beta, head_num,fusion,act),file=ff) \n",
    "    # print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\"  \\\n",
    "    #       % (overall_f1, auc,\\\n",
    "    #          precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "    #          recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "    #          revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "    #          ),\n",
    "    #          file=ff)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5. Comparison with XGBoost and Logistic Regression <a id='part5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1. Performance of XGBoost model\n",
    "Performance indicators;\n",
    "- precision = (number of seizures)/(number of inspections) --> targeting accuracy\n",
    "- recall = (number of seizures)/(number of actual frauds)\n",
    "- revenue_recall = (revenue from seizures)/(revenue from actual frauds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precision and Recall\n",
    "y_prob = test_pred\n",
    "for i in [99,98,95,90]:\n",
    "    # Find the ith value in ascending order.\n",
    "    threshold = np.percentile(y_prob, i)\n",
    "    print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "    precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "    recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "    revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "    print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the structured trees in txt file\n",
    "xgb_clf.get_booster().dump_model('xgb_model.txt', with_stats=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2. XGBoost + Logistic Regression model <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1. Deploy Xgboost+LR model \n",
    "Summary\n",
    "    1. Apply the pre-trained XGBoost model to train-, valid- and test-data. \n",
    "    2. For an individual import, the XGBoost model returns a list of leaf index of the multiple trees. \n",
    "    3. One-hot-encode leaf node.\n",
    "    4. The encoded leaf index is fed as an input into the logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Apply the pre-trained XGBoost model to train-, valid- and test-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost+LR model \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For an individual import, the XGBoost model returns a list of leaf index of the multiple trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 100 leaf nodes of the first import in the train-data\n",
    "X_train_leaves[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. One-hot-encode the leaf (Create unique set of 0s and 1s for individual leaf index).\n",
    "For instance, \n",
    "    - Leaf index 1: [100000...00]\n",
    "    - Leaf index 2: [010000...00]\n",
    "    - Leaf index 3: [001000...00]\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding for leaf index\n",
    "xgbenc = OneHotEncoder(categories=\"auto\")\n",
    "lr_trainx = xgbenc.fit_transform(X_train_leaves)\n",
    "lr_validx = xgbenc.transform(X_valid_leaves)\n",
    "lr_testx = xgbenc.transform(X_test_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Input the encoded leaf index into the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "print(\"Training Logistic regression model...\")\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(lr_trainx, xgb_trainy)\n",
    "test_pred = lr.predict_proba(lr_testx)[:,1]\n",
    "print(\"------Evaluating xgboost+LR model------\")\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "xgb_threshold,_ = find_best_threshold(lr, lr_validx, xgb_validy) # threshold was select from validation set\n",
    "xgb_f1 = find_best_threshold(lr, lr_testx, xgb_testy,best_thresh=xgb_threshold) # then applied on test set\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2. Performance of XGBoost + LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Precision and Recall\n",
    "y_prob = test_pred\n",
    "for i in [99,98,95,90]:\n",
    "    threshold = np.percentile(y_prob, i)\n",
    "    print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "    precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "    recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "    revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "    print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6. Practice of functions <a id='part6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge_attributes <a id='practice1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a small data for the demonstration.\n",
    "df_sample = df.sample(3)\n",
    "merge_attributes(df_sample, 'TARIFF.CODE','ORIGIN.CODE')\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can identify that new column \"TARIFF.CODE&ORIGIN.CODE\" has been created.  \n",
    "You can learn each step of the function by replicating the codes line by line as follows;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(3)\n",
    "print('Check what happens from \"iterables = [df[arg].astype(str) for arg in args]\"\\n=====')\n",
    "iterables = [df_sample[arg].astype(str) for arg in ['TARIFF.CODE','ORIGIN.CODE']] \n",
    "print(iterables)\n",
    "print('')\n",
    "print('Check what happens from \"zip(*iterables)\"\\n=====')\n",
    "print(list(zip(*iterables)))\n",
    "print('')\n",
    "print('What happens from \"fs = [''.join([v for v in var]) for var in zip(*iterables)]\" - part1\\n=====')\n",
    "print(list(var for var in zip(*iterables)))\n",
    "print('')\n",
    "print('What happens from \"fs = [''.join([v for v in var]) for var in zip(*iterables)]\" - part2\\n=====')\n",
    "fs = [''.join([v for v in var]) for var in zip(*iterables)]\n",
    "print(fs)\n",
    "print('')\n",
    "print('What happens from \"columnName = \\'&\\'.join([*args])\\'&\\'.join([*args])\"\\n=====')\n",
    "columnName = '&'.join(['TARIFF.CODE','ORIGIN.CODE'])\n",
    "print(columnName)\n",
    "print('')\n",
    "df_sample.loc[:, columnName] = fs\n",
    "print('df_sample[columnName]\\n=====')\n",
    "print(df_sample[columnName])\n",
    "\n",
    "# Now, we finish the line-by-line practice of the \"merge_attributes\" fuction, and delete temporarily created data. \n",
    "\n",
    "del df_sample, iterables, fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess <a id='practice2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(3)\n",
    "df_sample = preprocess(df_sample)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find_risk_profile <a id='practice3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(100)\n",
    "\n",
    "# We will identify top 10% of high risk importers in terms of its non-compliance records. \n",
    "find_risk_profile(df=df_sample, feature='IMPORTER.TIN', topk_ratio=0.1, adj=10, option='topk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tag_risky_profiles <a id='practice4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample data\n",
    "df_sample = df.sample(100)\n",
    "# Identify top 10% high-risk importer\n",
    "risk_profiles = find_risk_profile(df_sample, 'IMPORTER.TIN', 0.1, 10, option='topk')\n",
    "risk_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag \"RiskH\" to the top 10% high-risk importers\n",
    "aaa = tag_risky_profiles(df=df_sample, profile='IMPORTER.TIN', profiles=risk_profiles, option='topk')\n",
    "aaa.sort_values('RiskH.IMPORTER.TIN', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample, aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_leaf_idx <a id='practice5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = X_train_leaves[:3] # 100 trees in our xgboost model!\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves, total_leaves, new_leaf_index = process_leaf_idx(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new idex of leaf\n",
    "leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reference: {new_leaf_index: {tree_index: leaf_index}}\n",
    "new_leaf_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample, leaves, total_leaves, new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish <a id='practice6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,3))\n",
    "Mish_test = Mish()\n",
    "print(\"x: \", x,\"Mish(x): \", Mish_test(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FusionAttention <a id='practice7'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('0. Set an arbitrary imput data\\n=====')\n",
    "inputs_test = torch.randn(256,3,16)\n",
    "print(inputs_test.shape)\n",
    "dim_test=inputs_test.shape[-1]\n",
    "print('')\n",
    "print(\"1. query_project \\n==============\")\n",
    "query_project = nn.Linear(dim_test,dim_test)(inputs_test)\n",
    "print(query_project.shape)\n",
    "print('')\n",
    "print(\"2. query_project \\n==============\")\n",
    "query_project = F.leaky_relu(query_project)\n",
    "print(query_project.shape)\n",
    "print('')\n",
    "print(\"3. project_value \\n==============\")\n",
    "project_value = nn.Linear(dim_test,1)(query_project)\n",
    "print(project_value.shape)\n",
    "print('')\n",
    "print(\"4. attention_weight \\n==============\")\n",
    "attention_weight = torch.softmax(project_value, dim=1)\n",
    "print(attention_weight.shape)\n",
    "print('')\n",
    "print(\"5. attention_vec \\n==============\")\n",
    "attention_vec = inputs_test*attention_weight\n",
    "print(attention_vec.shape)\n",
    "print('')\n",
    "print(\"6. attention_vec \\n==============\")\n",
    "attention_vec = torch.sum(attention_vec, dim=1)\n",
    "print(attention_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention <a id='practice8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1. Set arbitrary \"query\" and \"key\"\\n=====')\n",
    "# \"query\" is the embedding layer of importer_id + HScode_id in the final model.\n",
    "# \"key\" is the embedding layer of leaf_id of 100 decision trees in the final model.\n",
    "query = torch.randn(256, 16) # (batch_size, n_embedding_dim)\n",
    "print('query:',query.shape)\n",
    "key = torch.randn(256, 100, 16) # (batch_size, n_trees, n_embedding_dim)\n",
    "print('key:',key.shape)\n",
    "print('')\n",
    "print('2. Transform \"quary\" to be merged with \"key\"\\n=====')\n",
    "dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "print('query:', query.shape)\n",
    "cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "print('cat_vector:',cat_vector.shape)\n",
    "print('')\n",
    "print('3. Neural Network\\n=====')\n",
    "project_vector = nn.Linear(32,16)(cat_vector)\n",
    "print('3-1. project_vector:',project_vector.shape)\n",
    "project_vector = torch.relu(project_vector)\n",
    "print('3-2. project_vector:',project_vector.shape)\n",
    "h = nn.Parameter(torch.rand(16,1))\n",
    "print('3-3. h:',h.shape)\n",
    "attention_alpha = torch.matmul(project_vector,h)\n",
    "attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "print('3-4. attention_weight:',attention_weight.shape)\n",
    "attention_vec = key * attention_weight # assumed 'key'=='value'\n",
    "print('3-5. attention_vec:',attention_vec.shape)\n",
    "print('')\n",
    "print('4. Result\\n=====')\n",
    "max_attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "print('4-1. max_attention_vec:',max_attention_vec.shape)\n",
    "mean_attention_vec = torch.mean(attention_vec,dim=1)\n",
    "print('4-2. mean_attention_vec:',mean_attention_vec.shape)\n",
    "sum_attention_vec = torch.sum(attention_vec,dim=1)\n",
    "print('4-3. sum_attention_vec:',sum_attention_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATE model <a id='practice9'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "head_num = 4\n",
    "\n",
    "print('1. Hyperparameters\\n=====')\n",
    "print('dim:',dim)\n",
    "print('leaf_num:',leaf_num)\n",
    "print('device:',device)\n",
    "print('item_size:',item_size)\n",
    "print('importer_size:',importer_size)\n",
    "print('')\n",
    "print('2. Toy data (batch)\\n=====\\none batch from \\'train_loader\\'')\n",
    "feature,uid,item_id,cls,reg = next(iter(train_loader))\n",
    "print('feature:',feature.shape)\n",
    "print('uid:',uid.shape)\n",
    "print('item_id:',item_id.shape)\n",
    "# other option\n",
    "#feature = train_loader.dataset.tensors[0][:256]\n",
    "#uid = train_loader.dataset.tensors[1][:256]\n",
    "#item_id = train_loader.dataset.tensors[2][:256]\n",
    "print('')\n",
    "print('3. Embedding of feature (cross-features, leaf_ids)\\n=====')\n",
    "leaf_vectors = nn.Embedding(leaf_num,dim)(feature)\n",
    "print('leaf_vectors:',leaf_vectors.shape)\n",
    "print('')\n",
    "print('4. Multi-Head Self-Attention (1st attention) \\n=====')\n",
    "# Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "# Apply multy head attention\n",
    "multiheadattention = MultiHeadAttention(dim,head_num).to(device)\n",
    "leaf_vectors = multiheadattention(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "print(leaf_vectors.shape)\n",
    "leaf_vectors = nn.LayerNorm((100,dim))(leaf_vectors)\n",
    "print(leaf_vectors.shape)\n",
    "print('')\n",
    "print('5. Embedding of importer_id\\n=====')\n",
    "importer_vector = nn.Embedding(importer_size,dim,padding_idx=0)(uid)\n",
    "print(importer_vector.shape)\n",
    "print('')\n",
    "print('6. Embedding of item_id(HScode)\\n=====')\n",
    "item_vector = nn.Embedding(item_size,dim,padding_idx=0)(item_id)\n",
    "print(item_vector.shape)\n",
    "print(' ')\n",
    "print('7. Multiply embeddings of importer_id and item_id\\n=====')\n",
    "query_vector = importer_vector * item_vector\n",
    "print(query_vector.shape)\n",
    "print(' ')\n",
    "print('8. Attention with leaf_id, importer_id and item_id (2nd attention)\\n=====')\n",
    "attention = Attention(dim,dim,\"sum\").to(device)\n",
    "set_vector, _ = attention(query_vector,leaf_vectors)\n",
    "print(set_vector.shape)\n",
    "print('')\n",
    "print('9. Concat the user_id, item_id and set(attention)_vector to fusion\\n=====')\n",
    "importer_vector=importer_vector.view(-1,1,dim)\n",
    "print('importer_vector:',importer_vector.shape)\n",
    "item_vector=item_vector.view(-1,1,dim) \n",
    "print('item_vector:',item_vector.shape)\n",
    "set_vector=set_vector.view(-1,1,dim)\n",
    "print('set_vector:',set_vector.shape)\n",
    "fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "print('fusion:', fusion.shape)\n",
    "print('')\n",
    "print('10. Fusion Attention\\n=====')\n",
    "fusion_att = FusionAttention(dim)\n",
    "fusion,_ = fusion_att(fusion) # fusion attention\n",
    "print('fusion:',fusion.shape)\n",
    "print('')\n",
    "print('11. Task-specific layers\\n=====')\n",
    "hidden = nn.Linear(dim,dim)(fusion)\n",
    "hidden = nn.LeakyReLU()(hidden)\n",
    "print('hidden:',hidden.shape)\n",
    "print('')\n",
    "print('12. output\\n=====')\n",
    "classification_output = torch.sigmoid(nn.Linear(dim,1)(hidden))\n",
    "print('classification_output:',classification_output.shape)\n",
    "regression_output = torch.relu(nn.Linear(dim,1)(hidden))\n",
    "print('regression_output:',regression_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
